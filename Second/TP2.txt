Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.
- You can include references to images or html files such as the reports generated with clusters. To do this, simply include this document in the folder with the reports or images and refer them in the text by the file name in an isolated line. For example, the line

test.png

refers to a test.png image file in the same folder as this document.

QUESTIONS:

Q1: Explain how you selected the best attributes for the clustering phase. In particular, indicate the visualization methods used to explore the extracted attributes and any statistical tests used.
R1: 

After the standardization, we see at the correlation matrix:

correlation_matrix.png

We can see that the first sixt feature represented in the 6x6 square in the top left are very uncorrelated between them, in fact they are the PCA trasformation of the space, and the PCA axis are Linearly independent.
Also the 6x6 square in the bottom rigth represent that the 6 features of the isomap are very uncorrelated. One thing that we notice is that the middle 6 variables are a bit correlated between them, they correspond to the feature from the tne.
Now the important things that guided us to the following steps is that between the first 6 (PCA) features and the last 6 (Isomap) features we have a correlation thet in general il bigger than 0.6.
So the PCA feature represent quite similar information about the data, than Isomap.
Than we remove the feature highly correlated. We can see a scatter maps of this features in the following image:

correlated_scatter_matrix.png

The last thing that we did is the Anova test twith the H0 hypothesis of equality of the means cector for the different class (in our case, clusters).
With this test we select the p_values of every features (resulting from the univariate test on each feature) and we calculate the critic value.
We than reject the features that satisfies the test, and so that are no informative because the means of the cluster are (for the Anova test) equals with 95% of confident (assuming alpha = 5%).
We end up with 10 features.


Q2: After selecting the attributes, did you standardize or normalize the values? Justify your decision.
R2: 

After selecting the attributes from the PCA, t-SNE and Isomap we make a test to evaluate what would be the best data transformation between standardize, normalize or max-min normalization. 
The test correspond to the train of 3 different linear SVM and a Anova test.
First we make an Anova test getting the f_score, converting it in "-log_10" scores and than normalizing the score to obtain a uniform sum (as probabilities).
We then train 3 linear SVMs and get the weigths that correspond to each features, separately. Also in this case we normalize to obain a uniform sum.
The graph is the following:

important_test_SVM.png

The best option seems to be the standardization because it puts "equal" empashys to each feature.


Q3: Explain how you found the neighborhood radius value (epsilon) for the DBSCAN algorithm by following the procedure described in the article "A density-based algorithm for discovering clusters in large spatial databases with noise".
R3: 


Q4: Examining the clusters generated by the DBSCAN algorithm with the value optimized by the method described in the article, do you think the result is adequate to cluster these images? Justify your answer.
R4: Also with the optimized parameter value, the results are still really bad, we cannot separate enough items. We think the reason for that is that this algorithm is not suitable with the problem.

Q5: Describe your analysis of the k (for K-Means) and epsilon (for DBSCAN) parameters using the internal and external indicators referred in the assignment page. Include the two plots of the indicator values â€‹â€‹(indicating the image name of each plot in one line in your answer) as a function of the k and epsilon parameters and explain how you chose the ranges for examining these parameters. Indicate, with justification, what conclusions you can draw from this analysis.
R5: The process of analysis of the epsilon value is explained above. Also, for our results the silhoutte values wasn't suitable in thi case. 
For the k-means, we simply did a for loop with a k-value from 2 to a kmax collecting all the results for each indicator.
Then we select the best one for the internal indicator, and the best one for the external.

Q6: Select some of the parameter values â€‹â€‹tested in question five and examine the corresponding clusters more closely, generating the HTML file with the images. Explain how you selected these parameter values, discuss the different options and propose a recommendation that could help the biologists' task of classifying cells and rejecting segmentation errors.
R6: We generate the 2 html for the kmeans the results, choosing the k value accordingly to the best k we get with internal, and external indicators. There are differences, based on the number of cluster created, but the advantages for the biologist choosing one or the other will not be so different.
We can say that, comparing this 2 clustering results, usually the more the cluster the better the results: but in both cases the segmentation errors are in one-two clusters. 

Q7: Discuss advantages or problems with these two algorithms (K-Means and DBSCAN) for the purpose of helping biologists to organize these images, considering your theoretical knowledge of these algorithms as well as the results you obtained in your work.
R7: We think that DBSCAN is not suitable for this problem, because it works well in a scenario where the clusters are dense, well separated, with some noise, and also with different variances; in our case, the clusters aren't separate enough to let it work well, in no configuration.
If we use to much features, we incur in the curse of dimentionality problem. With few features the results are better, but still it seems that this algorithm is not able to understand the real cluster, for different reasons.
With small epsilon values, the clustered items are just few, and everything else is considered noise. With bigger epsilon values we get a huge cluster with most of the elements inside.
The k-means works better. Here we don't have the curse of dimentionality problem, and so we can get better values results using more features.
Though the results contain mis-classification, this algorithm was able to put the segmentation errors in just one or two clusters. 
We can see some similarity in the created clusters, and some clusters can be error-free (only in the case of complete-cell clusters), and so these can be a good help for the biologist, but they will still need to separate at least the separated-cells from the mid-separated.

Q8: Consider other clustering algorithms embedded in the Scikit-Learn library. Choose one and apply it to this problem, optimizing the parameters you deem appropriate in the way that you find adequate. Justify your choices and discuss whether this option would yield more useful results for biologists.
R8: We tried AgglomerativeClustering and GaussianMixtures. The second seems to have better results in this scenario. We optimize the parameter that decide the number of mixture components.
Also with the best parameter, choosen accordingly to the internal and external indexes, the results wouldn't help the biologists.
If the choosed parameter is high enough (more then 5, 6) there could be some good clusters, but still too much noise.

Q9: (Optional) Implement the Bissecting K-Means hierarchical clustering algorithm as described in the assignment page and Lecture 19. Examine and discuss the results and their application to the problem of helping the biologists select and classify cell images.
R9: We haven't metrics to evaluate the Bissecting K-Means results, so we proceed by attempts. The results, looking at the html files, seem good. 
Selecting a number of clusters bigger then 3,4 we starts to get good result, also with just 3 features.
For instance (setting 3 features, and 6 clusters) we get 6 clusters with a nice intra-similarity, we have:
a cluster with an high percentage of garbage,
3 clusters without errors,
2 clusters with some errors.
So, the biologists can use this algorithm as a good help for selecting cells, they will just need to check few clusters, instead of each one.
